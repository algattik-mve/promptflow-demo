name: Deploy Prompts with Promptflow

on:
  workflow_dispatch:
  # workflow_run:
  #   workflows: ["run-eval-pf-pipeline"]
  #   branches: [main]
  #   types:
  #     - completed

permissions:
      id-token: write
      contents: read

env: 
  AZURE_RESOURCE_GROUP: ${{ vars.AZURE_RESOURCE_GROUP }}
  AZURE_ML_WORKSPACE: ${{ vars.AZURE_ML_WORKSPACE }}
  ENDPOINT_NAME: chat-with-patents-endpoint
  DEPLOYMENT_NAME: blue
  ENDPOINT_IDENTITY_ARM_ID: ${{ vars.ENDPOINT_IDENTITY_ARM_ID }}

concurrency:
  group: ${{ github.workflow }} # avoid concurrent runs

jobs:
  create-endpoint-and-deploy-pf:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: chat-with-patents
    # if: ${{ github.event_name == 'workflow_dispatch' || github.event.workflow_run.conclusion == 'success' }} 
    steps:
    - name: Check out repo
      uses: actions/checkout@v2
    - name: Azure login
      uses: azure/login@v1
      with:
        client-id: ${{ vars.AZURE_CLIENT_ID }}
        tenant-id: ${{ vars.AZURE_TENANT_ID }}
        subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}
    - name: Install az ml extension
      run: az extension add -n ml -y
    - name: Set Azure CLI defaults
      run: az configure --defaults group=${{env.AZURE_RESOURCE_GROUP}} workspace=${{env.AZURE_ML_WORKSPACE}}
    - name: Setup endpoint
      run: |
        az ml online-endpoint create $args || \
        az ml online-endpoint update $args
      env:
        # https://learn.microsoft.com/en-us/azure/machine-learning/how-to-access-resources-from-endpoints-managed-identities?view=azureml-api-2&tabs=user-identity-cli#create-an-online-endpoint
        args: >-
          --file deployment/endpoint.yaml
          --name ${{env.ENDPOINT_NAME}}
          --set identity.user_assigned_identities[0].resource_id=${{env.ENDPOINT_IDENTITY_ARM_ID}}
    - name: Get deployment name
      run: echo "ENDPOINT_IDENTITY_CLIENT_ID=$(az resource show --id ${{vars.ENDPOINT_IDENTITY_ARM_ID}} --query properties.clientId -o tsv)" >> "$GITHUB_ENV"
    - name: Configure deployment
      run: |
        yq -i e '.environment_variables.PRT_CONFIG_OVERRIDE="${{env.PRT_CONFIG_OVERRIDE}}"' deployment/deployment.yaml
        yq -i e '.environment_variables.AZURE_CLIENT_ID="${{env.ENDPOINT_IDENTITY_CLIENT_ID}}"' deployment/deployment.yaml
        cat deployment/deployment.yaml
      env:
        PRT_CONFIG_OVERRIDE: "deployment.subscription_id=${{ vars.AZURE_SUBSCRIPTION_ID }},deployment.resource_group=${{ env.AZURE_RESOURCE_GROUP }},deployment.workspace_name=${{ env.AZURE_ML_WORKSPACE }},deployment.endpoint_name=${{ env.ENDPOINT_NAME }},deployment.deployment_name=${{ env.DEPLOYMENT_NAME }}"
    - name: Setup deployment
      run: |
        az ml online-deployment create $args || \
        az ml online-deployment update $args
      env:
        args: >-
          --name ${{env.DEPLOYMENT_NAME}}
          --file deployment/deployment.yaml
          --endpoint-name ${{env.ENDPOINT_NAME}}
    - name: Updating endpoint traffic
      run: az ml online-endpoint update -n ${{env.ENDPOINT_NAME}} --traffic "${{env.DEPLOYMENT_NAME}}=100"
    - name: Check the status of the endpoint
      run: az ml online-endpoint show -n ${{env.ENDPOINT_NAME}}
    - name: Check the status of the deployment
      run: az ml online-deployment get-logs --name ${{env.DEPLOYMENT_NAME}} --endpoint-name ${{env.ENDPOINT_NAME}}
    - name: Invoke model
      run: az ml online-endpoint invoke --name ${{env.ENDPOINT_NAME}} --request-file deployment/sample-request.json